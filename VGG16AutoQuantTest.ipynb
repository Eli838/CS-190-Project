{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc5679-b34b-480b-96c7-c01b1c8c5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d7ca3c-74e7-4b68-ae8f-220fd3c03d5c",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb0a29-c9de-42be-9375-748444149300",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_lengths = [8,7,6,5,4,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a39c3-d403-452f-8d57-165bd494a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d97c0-7088-4769-9314-b9616f4bf353",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = Path('./figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f69351-69a2-4de4-b7ba-d44e2152e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae5115-68f1-4c23-bc2b-ef53fc859e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3a5e0-a40f-4267-88b3-2a06d6bd2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39017c41-3ccc-4504-bcfd-8d805a362d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "path = \"Automated_Output\"\n",
    "def create_model(bit_length):\n",
    "    vgg16 = models.vgg16(pretrained=False)\n",
    "    vgg16.classifier[4] = nn.Linear(4096,1024)\n",
    "    vgg16.classifier[6] = nn.Linear(1024,10)\n",
    "    vgg16.load_state_dict(torch.load(f'{path}/dt_quantized_model_bl_{bit_length}',map_location=device))\n",
    "    # vgg16.load_state_dict(torch.load(f'model_20240610_113550_DT_Quantized_NEW',map_location=device))\n",
    "\n",
    "    vgg16.eval()\n",
    "    return vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901b4ce-93b6-4eeb-a421-d33c0bc97245",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {7: 85.22, 6: 85.12, 5: 85.06, 4: 84.87, 3: 83.51, 2: 83.51}#, 1: 25.24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ee329-5a88-43ff-a891-a36c61c8a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(accuracies.keys()), list(accuracies.values()))\n",
    "# plt.gca().invert_xaxis()\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.xlabel('Number of bits')\n",
    "# plt.title('vgg16 Accuracy vs Number of DT Bits')\n",
    "# plt.savefig(fig_path / 'vgg16_accuracy_v_bits.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb73066-4699-433a-a5b1-9a218962e882",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd4726-be8d-49fd-a0aa-c247ce9a7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for bit_length in bit_lengths:\n",
    "    with open(f'{path}/dt_counts_bl_{bit_length}.pkl', 'rb') as f:\n",
    "        counts.append(pickle.load(f))\n",
    "        print(f'{bit_length} bit exponent counts: {counts[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af68ca-cf77-409d-bd09-84c41da96e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_models = {}\n",
    "for bit_length in bit_lengths:\n",
    "    my_models[bit_length] = create_model(bit_length);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62222afa-0628-40a0-b588-6b751687a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_unique_vals(model):\n",
    "    return np.unique(model.classifier[4].weight.flatten().detach().numpy()).shape\n",
    "for bit_length in bit_lengths:\n",
    "    print(f'Number of unique values for {bit_length} exponent dt quantization: {num_unique_vals(models[bit_length])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49019756-60ec-4918-b5ba-4b978a035e66",
   "metadata": {},
   "source": [
    "# Size Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd966420-7284-48aa-b0ce-3f5afdd008c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_size_change(alexnet, bit_length):\n",
    "    data_type_sizes = []\n",
    "    abs_max_sizes = []\n",
    "    count = 0\n",
    "    bits_in_fp32 = 8 + 23\n",
    "    for layer in [*alexnet.features,*alexnet.classifier]:\n",
    "        count += 1 \n",
    "        # curr_layer_path = curr_path / f'layer{count}.npy' \n",
    "        curr_data_size = 0\n",
    "        curr_abs_max_size = 0\n",
    "        try:\n",
    "            data_type_sizes.append(0)\n",
    "            abs_max_sizes.append(0)\n",
    "            if len(layer.weight.shape) == 4:\n",
    "                weights = layer.weight.detach()\n",
    "                # print(f'Layer {count}')# weights shape pre-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                for filter in range(0, weights.shape[0]):\n",
    "                    # print(f'Filter num {filter}')\n",
    "                    for channel in range(0, weights.shape[1]):\n",
    "                        # print(f'Channel num {channel}')\n",
    "                        # print(layer.weight[filter,channel])\n",
    "                        data_type_sizes[-1] += weights[filter,channel].numel()\n",
    "                        abs_max_sizes[-1] += weights[filter,channel].shape[0]\n",
    "                        # for row in range(0,weights.shape[2]):\n",
    "                        #     weights[filter,channel, row] = quantize_dequantize_dt(weights[filter,channel,row])\n",
    "                        # print(f'Finish window')\n",
    "                # print(f'Layer {count} weights shape post-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                # layer.weight = nn.parameter.Parameter(weights)\n",
    "                # print(f'Layer {count} weights shape post-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                # layer.weight = nn.parameter.Parameter(weights)\n",
    "            else:\n",
    "                weights = layer.weight.detach()\n",
    "                # print(f'Layer {count}')# weights shape pre-quantization: {layer.weight.shape}\\nWeights: {weights}')\n",
    "                data_type_sizes[-1] += weights.numel()\n",
    "                abs_max_sizes[-1] += weights.shape[0]\n",
    "                # print(weights.shape)\n",
    "                # for row in tqdm(range(0,weights.shape[0])):\n",
    "                #     weights[row] = quantize_dequantize_dt(weights[row])\n",
    "                # layer.weight = nn.parameter.Parameter(weights)\n",
    "                # print(f'Layer {count} weights shape post-quantization: {layer.weight.shape}\\nWeights: {weights}')\n",
    "                # print(layer.weight)\n",
    "        except (TypeError, AttributeError):\n",
    "            pass\n",
    "    return {'data_type_counts': np.array(data_type_sizes), 'data_type_sizes': np.array(data_type_sizes) * bit_length, 'abs_max_counts': np.array(abs_max_sizes), 'abs_max_sizes': np.array(abs_max_sizes) * bits_in_fp32,\n",
    "           'data_type_sizes_original': np.array(data_type_sizes) * bits_in_fp32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c071df-2735-45c6-80c7-e14691112f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_to_mb(bits):\n",
    "    return bits / 8000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674c076-22bf-4aed-9cc4-6e4fc9b98560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_size(model, bit_len):\n",
    "    bits_in_fp32 = 8 + 23\n",
    "    results = estimate_model_size_change(model, bit_len)\n",
    "    data_type_size = results['data_type_sizes'].sum()\n",
    "    abs_max_size = results['abs_max_sizes'].sum()\n",
    "    dt_counts = results['data_type_counts'].sum()\n",
    "    display(Markdown(f'## {bit_len} bits'))\n",
    "    print(f'{data_type_size} bits to represent the {bit_len} quantized dt and {abs_max_size} bits to represent the maxes')\n",
    "    print(f'{bits_to_mb(data_type_size)} mb to represent the {bit_len} quantized dt and {bits_to_mb(abs_max_size)} mb to represent the maxes')\n",
    "    print(f'\\nOriginal Model Size: {bits_to_mb( dt_counts * bits_in_fp32)} MB')\n",
    "    print(f'Quantized Model Size: {bits_to_mb( data_type_size +abs_max_size)} MB')\n",
    "    print(f'This is a {bits_to_mb( dt_counts * bits_in_fp32) / (bits_to_mb(data_type_size)+bits_to_mb(abs_max_size))}x decrease in size')\n",
    "    return (bits_to_mb(data_type_size)+bits_to_mb(abs_max_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338e03d-825b-47c9-926b-07b7812a1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_sizes = []\n",
    "for bit_length in bit_lengths:\n",
    "    compressed_sizes.append(compute_model_size(create_model(bit_length), bit_length))\n",
    "    display(Markdown('---'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057bb8e5-29c6-458c-b01e-4a7c1ecc5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bit_lengths,compressed_sizes)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.xlabel('Number of Bits')\n",
    "plt.ylabel('VGG16 Weights Size (MB)')\n",
    "plt.title('VGG16 Weights Size vs Number of Bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377897bc-a19b-4546-a76e-b007f6d13b69",
   "metadata": {},
   "source": [
    "# MSE Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e30e2-0d5d-4c69-87b3-256d90a9ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and Concatenate Entire Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ba859-e04e-4857-9fd5-e17aa3f1c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_model(model):\n",
    "    flattened_model = torch.tensor([])\n",
    "    for layer in [*model.features,*model.classifier]:\n",
    "        try:\n",
    "            weights = layer.weight.detach()\n",
    "            flattened_model = torch.concatenate((flattened_model, weights.flatten()))\n",
    "        except (TypeError, AttributeError):\n",
    "            pass\n",
    "    return flattened_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549de63-6f46-4822-8a19-4b4bba85db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "device = 'cpu'\n",
    "vgg16 = models.vgg16(pretrained=False)\n",
    "model_path = 'checkpoints/finetuned_vgg16_9'\n",
    "vgg16.classifier[4] = nn.Linear(4096,1024)\n",
    "vgg16.classifier[6] = nn.Linear(1024,10)\n",
    "vgg16.load_state_dict(torch.load(model_path))\n",
    "vgg16.to(device)\n",
    "output = ''\n",
    "for bit_length in bit_lengths:\n",
    "    model = create_model(bit_length)\n",
    "    flattened_vgg16 = flatten_model(vgg16)\n",
    "    flattened_quantized_vgg16 = flatten_model(model)\n",
    "    model_size = flattened_vgg16.shape[0]\n",
    "    MSE = torch.sum(torch.pow(flattened_vgg16 - flattened_quantized_vgg16, 2)) / model_size\n",
    "    # MAE = torch.sum(torch.abs(flattened_vgg16 - flattened_quantized_vgg16)) / model_size\n",
    "    print(f'{bit_length} model MSE: {MSE}')\n",
    "    output += f'{bit_length} model MSE: {MSE}\\n'\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f76fe2-a2ba-404d-b880-b2bc5572064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c76131-abef-40de-b0bd-f4dd925bc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "USE_DT_FILE = False\n",
    "Test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b82153a-883e-4e0b-824e-a95acfedc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2824be-ba9e-4b6e-a513-9cc9c9e3a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be2685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe21ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "def create_model():\n",
    "    alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "    \n",
    "    alexnet.classifier[4] = nn.Linear(4096,1024)\n",
    "    alexnet.classifier[6] = nn.Linear(1024,10)\n",
    "    \n",
    "    alexnet.load_state_dict(torch.load('model_20240611_201336_SE_Quantized_NEW',map_location=device)) # model_20240603_151633_final_frozen_alexnet\n",
    "    alexnet.eval()\n",
    "    return alexnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb18f0-6bc0-4ab7-8cb7-4dc68b6e3881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c9bbcd5-9d72-478a-a992-923231fe51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import struct\n",
    "import torch.nn.init as init\n",
    "\n",
    "EXP_COUNT = {0:0,\n",
    "             1:0,\n",
    "             2:0,\n",
    "             3:0,\n",
    "             4:0,\n",
    "             5:0,\n",
    "             6:0,\n",
    "             7:0,\n",
    "            }\n",
    "            \n",
    "# input : tensor output: binary string\n",
    "def binary(num):\n",
    "  return ''.join('{:0>8b}'.format(c) for c in struct.pack('!f', num))\n",
    "\n",
    "#input: mantissa bitstring\n",
    "#output: float value\n",
    "def calc_mantissa(mantissa):\n",
    "  res = 0\n",
    "  for k in range(len(mantissa)):\n",
    "      if mantissa[k] == '1':\n",
    "        res += 2**(-k-1)\n",
    "  return res\n",
    "\n",
    "#input exp: bitstring\n",
    "# new_exp_len: new length of exp\n",
    "def calc_exp(exp, new_exp_len):\n",
    "  limit = 2**(new_exp_len) - 1\n",
    "  # if exp is more than new_exp_len limit, truncate to new_exp_len limit.\n",
    "  bias = 2**(len(exp)-1) - 1\n",
    "  val = int(exp,2) - bias\n",
    "  if val > limit:\n",
    "      return limit\n",
    "  if val < -limit + 1:\n",
    "      return -limit + 1\n",
    "      pass\n",
    "  return val\n",
    "\n",
    "\n",
    "def round_fp8(x, exp = 4):\n",
    "  '''\n",
    "  Quantizes input tensor to FP8 data format\n",
    "  inputs  x:      original tensor\n",
    "          exp:    number of bits used for exponent field\n",
    "                  e.g. E5M2 has 5 exp bits, E4M3 has 4 exp bits\n",
    "  output  x_32:   quantized tensor\n",
    "  '''\n",
    "\n",
    "  x_fp8 = copy.deepcopy(x)\n",
    "\n",
    "\n",
    "  result = 1.0\n",
    "  bin_str = binary(x)\n",
    "\n",
    "  bin_mantissa = bin_str[9:32]\n",
    "  res_mantissa = bin_mantissa[:7-exp]\n",
    "  result += calc_mantissa(res_mantissa)\n",
    "\n",
    "  bin_exp = bin_str[1:9]\n",
    "  exp_int = calc_exp(bin_exp, exp)\n",
    "  result *= 2**exp_int\n",
    "\n",
    "  if bin_str[0] == '1':\n",
    "    result *= -1\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "def bisection_quantization(num, bits = 7):\n",
    "    if bits == 0:\n",
    "        return 0.1\n",
    "    val = abs(num)\n",
    "    inversed_bits = []\n",
    "    # Bisection tree quantization\n",
    "    range_min, range_max = 0, 1\n",
    "    for p in range(bits):\n",
    "        p += 1\n",
    "        bit_val = 2**(-p)\n",
    "        if val >= bit_val:\n",
    "            inversed_bits.append(1)\n",
    "            val -= bit_val\n",
    "        else:\n",
    "            inversed_bits.append(0)\n",
    "\n",
    "    quantized_val = 0\n",
    "    for k, bit in enumerate(inversed_bits):\n",
    "        if bit:\n",
    "            quantized_val += 2**-(k + 1)\n",
    "    return quantized_val\n",
    "\n",
    "#the input is normalized tensor x,\n",
    "def round_dt8(x, exp = 4, num_bits = 8):\n",
    "  val = copy.deepcopy(x)\n",
    "\n",
    "  sign_bit = 0 if val >= 0 else 1\n",
    "  val = abs(val)\n",
    "  exp_bits = 0\n",
    "  while val < 0.1:\n",
    "      val *= 10\n",
    "      exp_bits += 1\n",
    "\n",
    "  bs_bits = max(0, num_bits - 2 - exp_bits)\n",
    "  exp_bits = min(num_bits -1, exp_bits)\n",
    "  EXP_COUNT[exp_bits] += 1\n",
    "\n",
    "  if exp_bits == 0:\n",
    "      quantized_val = bisection_quantization(val, 7)\n",
    "  elif exp_bits >= num_bits - 2:\n",
    "      quantized_val = 0.0\n",
    "  else:\n",
    "      quantized_val = bisection_quantization(val, bs_bits)\n",
    "\n",
    "  quantized_val = quantized_val if sign_bit == 0 else -quantized_val\n",
    "  quantized_val *= 10**(-exp_bits)\n",
    "  return quantized_val\n",
    "\n",
    "\n",
    "def quantize_rowwise(x: torch.Tensor, dt = False):\n",
    "  '''Takes in a (2d) tensor and returns quantized array\n",
    "  DT = if you use the dynamic tree quantization. False is using fp8\n",
    "  tensor.view( shape[0],-1) can reshape a 3d tensor to 2d. that should work'''\n",
    "  abso = torch.abs(x)\n",
    "  output_maxs  = torch.max(abso,1)[0].unsqueeze(-1)\n",
    "  output = x  / output_maxs[None,:]\n",
    "  if not dt:\n",
    "      output.apply_(round_fp8)\n",
    "  else:\n",
    "      output.apply_(round_dt8)\n",
    "  return torch.squeeze(output), output_maxs\n",
    "\n",
    "def dequantize_rowwise(x: torch.Tensor, state_x: torch.Tensor):\n",
    "  '''Dequantizes the tensor given the maxes'''\n",
    "  output = x * state_x\n",
    "  return output\n",
    "\n",
    "def init_weights(m, in_std = 1.0):\n",
    "  if isinstance(m, nn.Linear):\n",
    "      init.normal_(m.weight, mean=0.0, std=in_std)\n",
    "      if m.bias is not None:\n",
    "          init.zeros_(m.bias)\n",
    "\n",
    "def measure_quantization_error(original_tensor, dequantized_tensor):\n",
    "  abs_error = torch.abs(original_tensor - dequantized_tensor)\n",
    "  return torch.mean(abs_error), abs_error\n",
    "\n",
    "\n",
    "def quantize_stable_embedding(x, batch_size, dt = False):\n",
    "  '''Qunatizes the given array\n",
    "  Batch size must be a divisor of the array size\n",
    "  Returns the quantized array, the maximums, and the indexes\n",
    "  DT  = false, means using fp8\n",
    "  Dt = true, means using the dynamic tree\n",
    "  '''\n",
    "  if (x.numel() % batch_size != 0):\n",
    "    print(\"Invalid batch size. Batch size should be a divisor of \" + str(x.numel()))\n",
    "    return\n",
    "\n",
    "  flatarg = torch.argsort(torch.abs(x.flatten()))\n",
    "  indexing = flatarg.reshape((x.numel()//batch_size,batch_size))\n",
    "\n",
    "  reshapedx = x.flatten()[indexing]\n",
    "\n",
    "  output, maxes = quantize_rowwise(reshapedx,dt)\n",
    "\n",
    "  return output.reshape(x.shape), maxes, indexing\n",
    "\n",
    "def dequantize_stable_embedding(input, maxes, indexing):\n",
    "  '''Takes the quantized matrices and dequantizes them by multiplying the normalized and maxes\n",
    "  Then uses the indexes to place the dequantized values back into their original spots\n",
    "  returns dequantized values in the right positions\n",
    "  Takes in the quantized array, the array maximums, and the indexes'''\n",
    "  outreshape = input.reshape(indexing.shape)\n",
    "\n",
    "  dequant = dequantize_rowwise(outreshape, maxes).flatten()\n",
    "  dequant[indexing.flatten()] = dequant.clone()\n",
    "  return dequant.reshape(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9148c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_stable_embedding(x, batch_size, dt = False):\n",
    "  '''Qunatizes the given array\n",
    "  Batch size must be a divisor of the array size\n",
    "  Returns the quantized array, the maximums, and the indexes\n",
    "  DT  = false, means using fp8\n",
    "  Dt = true, means using the dynamic tree\n",
    "  '''\n",
    "  if (x.numel() % batch_size != 0):\n",
    "    print(\"Invalid batch size. Batch size should be a divisor of \" + str(x.numel()))\n",
    "    return\n",
    "\n",
    "  flatarg = torch.argsort(torch.abs(x.flatten()))\n",
    "  indexing = flatarg.reshape((x.numel()//batch_size,batch_size))\n",
    "\n",
    "  reshapedx = x.flatten()[indexing]\n",
    "\n",
    "  output, maxes = quantize_rowwise(reshapedx,dt)\n",
    "\n",
    "  return output.reshape(x.shape), maxes, indexing\n",
    "\n",
    "def dequantize_stable_embedding(input, maxes, indexing):\n",
    "  '''Takes the quantized matrices and dequantizes them by multiplying the normalized and maxes\n",
    "  Then uses the indexes to place the dequantized values back into their original spots\n",
    "  returns dequantized values in the right positions\n",
    "  Takes in the quantized array, the array maximums, and the indexes'''\n",
    "  outreshape = input.reshape(indexing.shape)\n",
    "\n",
    "  dequant = dequantize_rowwise(outreshape, maxes).flatten()\n",
    "  dequant[indexing.flatten()] = dequant.clone()\n",
    "  return dequant.reshape(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76bad044-eff5-4297-bd72-9e375c3aba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_dequantize_se(mat):\n",
    "    testing, max, indexing = quantize_stable_embedding(mat, mat.shape[0],dt = True)\n",
    "    return dequantize_stable_embedding(testing, max, indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73951842",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f4528-c293-491f-9438-d70633a66fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path('./Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf9ebd-6495-4059-a8e7-5e7ede90ff7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d6c027-60aa-42c8-9fc2-9d59c4a999b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Elijah/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\Elijah\\anaconda3\\envs\\cs190-project\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Elijah\\anaconda3\\envs\\cs190-project\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Layer 1\n",
      "1.Layer 1 finished quantization\n",
      "1.Layer 4\n",
      "1.Layer 4 finished quantization\n",
      "1.Layer 7\n",
      "1.Layer 7 finished quantization\n",
      "1.Layer 9\n",
      "1.Layer 9 finished quantization\n",
      "1.Layer 11\n",
      "1.Layer 11 finished quantization\n",
      "2. Layer 15\n",
      "2.Layer 15 finished quantization\n",
      "2. Layer 18\n",
      "2.Layer 18 finished quantization\n",
      "2. Layer 20\n",
      "2.Layer 20 finished quantization\n"
     ]
    }
   ],
   "source": [
    "alexnet = create_model()\n",
    "quantized_lst = []\n",
    "if not DEBUG:\n",
    "    count = 0\n",
    "    for layer in [*alexnet.features,*alexnet.classifier]:\n",
    "        count += 1\n",
    "        try:\n",
    "            if len(layer.weight.shape) == 4:\n",
    "                weights = layer.weight.detach()\n",
    "                print(f'1.Layer {count}')# weights shape pre-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                for filter in range(0, weights.shape[0]):\n",
    "                    # print(f'Filter num {filter}')\n",
    "                    for channel in range(0, weights.shape[1]):\n",
    "                        # print(f'Channel num {channel}')\n",
    "                        # print(layer.weight[filter,channel])\n",
    "                        weights[filter,channel] = quantize_dequantize_se(weights[filter,channel])\n",
    "                        # for row in range(0,weights.shape[2]):\n",
    "                        #     weights[filter,channel, row] = quantize_dequantize_dt(weights[filter,channel,row])\n",
    "                        # print(f'Finish window')\n",
    "                # print(f'Layer {count} weights shape post-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                # layer.weight = nn.parameter.Parameter(weights)\n",
    "                # print(f'2.Layer {count} weights shape post-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                layer.weight = nn.parameter.Parameter(weights)\n",
    "                quantized_lst.append(count)\n",
    "                print(f'1.Layer {count} finished quantization')\n",
    "            else:\n",
    "                # print(\"##1\",layer.weight)\n",
    "                weights = layer.weight.detach()\n",
    "                print(f'2. Layer {count}')# weights shape pre-quantization: {layer.weight.shape}\\nWeights: {weights}')\n",
    "                weights = quantize_dequantize_se(weights)\n",
    "                # for row in tqdm(range(0,weights.shape[0])):\n",
    "                #     weights[row] = quantize_dequantize_dt(weights[row])\n",
    "                # print(\"##2\",weights)\n",
    "                layer.weight = nn.parameter.Parameter(weights)\n",
    "                # print(f'Layer {count} weights shape post-quantization: {layer.weight.shape}\\nWeights: {weights}')\n",
    "                # print(layer.weight)\n",
    "                # print(\"##3\",layer.weight)\n",
    "                quantized_lst.append(count)\n",
    "                print(f'2.Layer {count} finished quantization')\n",
    "\n",
    "        except (TypeError, AttributeError):\n",
    "            pass\n",
    "else:\n",
    "    count = 0\n",
    "    for layer in alexnet.classifier:\n",
    "        count += 1\n",
    "        try:\n",
    "            if len(layer.weight.shape) == 4:\n",
    "                weights = layer.weight.detach()\n",
    "                print(f'Layer {count} weights shape pre-quantization: {weights.shape}')\n",
    "                \n",
    "                for filter in range(0, layer.weight.shape[0]):\n",
    "                    for channel in range(0, layer.weight.shape[1]):\n",
    "                        # print(layer.weight[filter,channel])\n",
    "                        weights[filter,channel] = quantize_dequantize_se(weights[filter,channel])\n",
    "                print(f'Layer {count} weights shape post-quantization: {weights.shape}')\n",
    "                layer.weight = nn.parameter.Parameter(weights)\n",
    "            else:\n",
    "                print(f'In else loop')\n",
    "                print(f'Layer {count} weights shape pre-quantization: {layer.weight.shape}')\n",
    "                intermediate = quantize_dequantize_se(layer.weight.detach())\n",
    "                print(intermediate.shape)\n",
    "                layer.weight = nn.parameter.Parameter(intermediate)\n",
    "                print(f'Layer {count} weights shape post-quantization: {layer.weight.shape}')\n",
    "                # print(layer.weight)\n",
    "        except (TypeError, AttributeError):\n",
    "            pass\n",
    "\n",
    "# for layer in [*alexnet.features,*alexnet.classifier]:\n",
    "#     try:\n",
    "#         if len(layer.weight.shape) == 4:\n",
    "#         for filter in range(0, layer.weight.shape[0]):\n",
    "#             for channel in \n",
    "#         layer.weight = nn.parameter.Parameter(quantize_dequantize_dt(layer.weight.detach()))\n",
    "#     except (TypeError, AttributeError):\n",
    "#         print(layer)\n",
    "\n",
    "# for layer in [*alexnet.features,*alexnet.classifier]:\n",
    "#     try:\n",
    "#         if len(layer.weight.shape) == 4:\n",
    "#         for filter in range(0, layer.weight.shape[0]):\n",
    "#             for channel in \n",
    "#         layer.weight = nn.parameter.Parameter(quantize_dequantize_dt(layer.weight.detach()))\n",
    "#     except (TypeError, AttributeError):\n",
    "#         print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdbfe735-8521-4dff-8f9c-e61ea4d67a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Elijah/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\Elijah\\anaconda3\\envs\\cs190-project\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Elijah\\anaconda3\\envs\\cs190-project\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = create_model()\n",
    "alexnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf6ff5bc-f132-40a9-922b-79684bba442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:17<00:00, 144.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 85.19 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DT QUANTIZATION\n",
    "if Test:\n",
    "    #Testing Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = alexnet(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35c1422e-6b56-42cc-87a3-3a6ed13d4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab861791-a886-43a1-9c9c-ddff5838c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_{}_{}'.format(timestamp, \"SE_Quantized_NEW\")\n",
    "torch.save(alexnet.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76b0b3-a0cc-41e8-a44a-63f7f852712c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f76fe2-a2ba-404d-b880-b2bc5572064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <E459C462-F863-3A5A-AC9F-FD77B14BE845> /Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c76131-abef-40de-b0bd-f4dd925bc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "USE_DT_FILE = False\n",
    "Test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b82153a-883e-4e0b-824e-a95acfedc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2824be-ba9e-4b6e-a513-9cc9c9e3a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be2685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('mps')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe21ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "vgg16 = models.vgg16(pretrained=False)\n",
    "model_path = 'checkpoints/finetuned_vgg16_9'\n",
    "vgg16.classifier[4] = nn.Linear(4096,1024)\n",
    "vgg16.classifier[6] = nn.Linear(1024,10)\n",
    "vgg16.load_state_dict(torch.load(model_path))\n",
    "vgg16.to(device)\n",
    "\n",
    "vgg16.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9bbcd5-9d72-478a-a992-923231fe51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import struct\n",
    "import torch.nn.init as init\n",
    "\n",
    "# input : tensor output: binary string\n",
    "def binary(num):\n",
    "  return ''.join('{:0>8b}'.format(c) for c in struct.pack('!f', num))\n",
    "\n",
    "#input: mantissa bitstring\n",
    "#output: float value\n",
    "def calc_mantissa(mantissa):\n",
    "  res = 0\n",
    "  for k in range(len(mantissa)):\n",
    "      if mantissa[k] == '1':\n",
    "        res += 2**(-k-1)\n",
    "  return res\n",
    "\n",
    "#input exp: bitstring\n",
    "# new_exp_len: new length of exp\n",
    "def calc_exp(exp, new_exp_len):\n",
    "  limit = 2**(new_exp_len) - 1\n",
    "  # if exp is more than new_exp_len limit, truncate to new_exp_len limit.\n",
    "  bias = 2**(len(exp)-1) - 1\n",
    "  val = int(exp,2) - bias\n",
    "  if val > limit:\n",
    "      return limit\n",
    "  if val < -limit + 1:\n",
    "      return -limit + 1\n",
    "      pass\n",
    "  return val\n",
    "\n",
    "\n",
    "def round_fp8(x, exp = 4):\n",
    "  '''\n",
    "  Quantizes input tensor to FP8 data format\n",
    "  inputs  x:      original tensor\n",
    "          exp:    number of bits used for exponent field\n",
    "                  e.g. E5M2 has 5 exp bits, E4M3 has 4 exp bits\n",
    "  output  x_32:   quantized tensor\n",
    "  '''\n",
    "\n",
    "  x_fp8 = copy.deepcopy(x)\n",
    "\n",
    "\n",
    "  result = 1.0\n",
    "  bin_str = binary(x)\n",
    "\n",
    "  bin_mantissa = bin_str[9:32]\n",
    "  res_mantissa = bin_mantissa[:7-exp]\n",
    "  result += calc_mantissa(res_mantissa)\n",
    "\n",
    "  bin_exp = bin_str[1:9]\n",
    "  exp_int = calc_exp(bin_exp, exp)\n",
    "  result *= 2**exp_int\n",
    "\n",
    "  if bin_str[0] == '1':\n",
    "    result *= -1\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "def bisection_quantization(num, bits = 7):\n",
    "  val = abs(num)\n",
    "  inversed_bits = []\n",
    "  # Bisection tree quantization\n",
    "  range_min, range_max = 0, 1\n",
    "  for _ in range(bits):\n",
    "      mid = (range_min + range_max) / 2\n",
    "      if val >= mid:\n",
    "          inversed_bits.append(1)\n",
    "          range_min = mid\n",
    "      else:\n",
    "          inversed_bits.append(0)\n",
    "          range_max = mid\n",
    "\n",
    "  quantized_val = 0\n",
    "  for k, bit in enumerate(inversed_bits):\n",
    "      if bit:\n",
    "          quantized_val += 2**-(k + 1)\n",
    "\n",
    "  return quantized_val\n",
    "\n",
    "#the input is normalized tensor x,\n",
    "def round_dt8(x, exp = 4):\n",
    "  val = copy.deepcopy(x)\n",
    "  num_levels = 2 ** (7)\n",
    "\n",
    "\n",
    "  sign_bit = 0 if val >= 0 else 1\n",
    "  val = abs(val)\n",
    "  exp_bits = 0\n",
    "  while val < 0.1:\n",
    "      val *= 10\n",
    "      exp_bits += 1\n",
    "\n",
    "  bs_bits = max(0, 6 - exp_bits)\n",
    "  exp_bits = min(7, exp_bits)\n",
    "\n",
    "  if exp_bits == 0:\n",
    "      quantized_val = bisection_quantization(val, 7)\n",
    "  elif exp_bits >= 6:\n",
    "      quantized_val = val\n",
    "  else:\n",
    "      quantized_val = bisection_quantization(val, bs_bits)\n",
    "\n",
    "  quantized_val = quantized_val if sign_bit == 0 else -quantized_val\n",
    "  quantized_val *= 10**(-exp_bits)\n",
    "  return quantized_val\n",
    "\n",
    "\n",
    "def quantize_rowwise(x: torch.Tensor, dt = False):\n",
    "  abso = torch.abs(x)\n",
    "  output_maxs  = torch.max(abso,1)[0].unsqueeze(-1)\n",
    "  output = x  / output_maxs[None,:]\n",
    "  if not dt:\n",
    "      output.apply_(round_fp8)\n",
    "  else:\n",
    "      output.apply_(round_dt8)\n",
    "  return torch.squeeze(output), output_maxs\n",
    "\n",
    "def dequantize_rowwise(x: torch.Tensor, state_x: torch.Tensor):\n",
    "  output = x * state_x\n",
    "  return output\n",
    "\n",
    "def init_weights(m, in_std = 1.0):\n",
    "  if isinstance(m, nn.Linear):\n",
    "      init.normal_(m.weight, mean=0.0, std=in_std)\n",
    "      if m.bias is not None:\n",
    "          init.zeros_(m.bias)\n",
    "\n",
    "def measure_quantization_error(original_tensor, dequantized_tensor):\n",
    "  abs_error = torch.abs(original_tensor - dequantized_tensor)\n",
    "  return torch.mean(abs_error), abs_error\n",
    "\n",
    "def quantize_stable_embedding(x, batch_size, dt = False):\n",
    "  if (x.numel() % batch_size != 0):\n",
    "    print(\"Invalid batch size. Batch size should be a divisor of \" + str(x.numel()))\n",
    "    return \n",
    "\n",
    "  flatarg = torch.argsort(x.flatten())\n",
    "  indexing = flatarg.reshape((x.numel()//batch_size,batch_size))\n",
    "\n",
    "  reshapedx = x.flatten()[indexing]\n",
    "  output, maxes = quantize_rowwise(reshapedx,dt)\n",
    "\n",
    "  return output.reshape(x.shape), maxes, indexing\n",
    "\n",
    "def dequantize_stable_embedding(input, maxes, indexing):\n",
    "  outreshape = input.reshape(indexing.shape)\n",
    "\n",
    "  dequant = dequantize_rowwise(outreshape, maxes)\n",
    "  return dequant.reshape(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9148c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_stable_embedding(x, batch_size, dt = False):\n",
    "  '''Qunatizes the given array\n",
    "  Batch size must be a divisor of the array size\n",
    "  Returns the quantized array, the maximums, and the indexes\n",
    "  DT  = false, means using fp8\n",
    "  Dt = true, means using the dynamic tree\n",
    "  '''\n",
    "  if (x.numel() % batch_size != 0):\n",
    "    print(\"Invalid batch size. Batch size should be a divisor of \" + str(x.numel()))\n",
    "    return\n",
    "\n",
    "  flatarg = torch.argsort(torch.abs(x.flatten()))\n",
    "  indexing = flatarg.reshape((x.numel()//batch_size,batch_size))\n",
    "\n",
    "  reshapedx = x.flatten()[indexing]\n",
    "\n",
    "  output, maxes = quantize_rowwise(reshapedx,dt)\n",
    "\n",
    "  return output.reshape(x.shape), maxes, indexing\n",
    "\n",
    "def dequantize_stable_embedding(input, maxes, indexing):\n",
    "  '''Takes the quantized matrices and dequantizes them by multiplying the normalized and maxes\n",
    "  Then uses the indexes to place the dequantized values back into their original spots\n",
    "  returns dequantized values in the right positions\n",
    "  Takes in the quantized array, the array maximums, and the indexes'''\n",
    "  outreshape = input.reshape(indexing.shape)\n",
    "\n",
    "  dequant = dequantize_rowwise(outreshape, maxes).flatten()\n",
    "  dequant[indexing.flatten()] = dequant.clone()\n",
    "  return dequant.reshape(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76bad044-eff5-4297-bd72-9e375c3aba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_dequantize_se(mat,output_path,count):\n",
    "    testing, max, indexing = quantize_stable_embedding(mat, mat.shape[0],dt = True)\n",
    "    np.save(f'{output_path}/layer{count}_dt_max.npy',testing)\n",
    "    np.save(f'{output_path}/layer{count}_qweights.npy',max)\n",
    "    np.save(f'{output_path}/layer{count}_qweights.npy',indexing)\n",
    "    return dequantize_stable_embedding(testing, max, indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73951842",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d6c027-60aa-42c8-9fc2-9d59c4a999b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Layer 1\n",
      "1.Layer 1 finished quantization\n",
      "1.Layer 3\n",
      "1.Layer 3 finished quantization\n",
      "1.Layer 6\n",
      "1.Layer 6 finished quantization\n",
      "1.Layer 8\n",
      "1.Layer 8 finished quantization\n",
      "1.Layer 11\n",
      "1.Layer 11 finished quantization\n",
      "1.Layer 13\n",
      "1.Layer 13 finished quantization\n",
      "1.Layer 15\n",
      "1.Layer 15 finished quantization\n",
      "1.Layer 18\n",
      "1.Layer 18 finished quantization\n",
      "1.Layer 20\n",
      "1.Layer 20 finished quantization\n",
      "1.Layer 22\n",
      "1.Layer 22 finished quantization\n",
      "1.Layer 25\n",
      "1.Layer 25 finished quantization\n",
      "1.Layer 27\n",
      "1.Layer 27 finished quantization\n",
      "1.Layer 29\n",
      "1.Layer 29 finished quantization\n",
      "2. Layer 32\n",
      "2.Layer 32 finished quantization\n",
      "2. Layer 35\n",
      "2.Layer 35 finished quantization\n",
      "2. Layer 36\n",
      "2.Layer 36 finished quantization\n",
      "2. Layer 38\n",
      "2.Layer 38 finished quantization\n"
     ]
    }
   ],
   "source": [
    "quantized_lst = []\n",
    "if not DEBUG:\n",
    "    count = 0\n",
    "    curr_path = 'stable_embedding' \n",
    "    for layer in [*vgg16.features,*vgg16.classifier]:\n",
    "        layer.requires_grad = False\n",
    "        count += 1\n",
    "        try:\n",
    "            if len(layer.weight.shape) == 4:\n",
    "                weights = layer.weight.detach()\n",
    "                print(f'1.Layer {count}')# weights shape pre-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                for filter in range(0, weights.shape[0]):\n",
    "                    # print(f'Filter num {filter}')\n",
    "                    for channel in range(0, weights.shape[1]):\n",
    "                        # print(f'Channel num {channel}')\n",
    "                        # print(layer.weight[filter,channel])\n",
    "                        weights[filter,channel] = quantize_dequantize_se(weights[filter,channel],curr_path,count)\n",
    "                        # for row in range(0,weights.shape[2]):\n",
    "                        #     weights[filter,channel, row] = quantize_dequantize_dt(weights[filter,channel,row])\n",
    "                        # print(f'Finish window')\n",
    "                # print(f'Layer {count} weights shape post-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                # layer.weight = nn.parameter.Parameter(weights)\n",
    "                # print(f'2.Layer {count} weights shape post-quantization: {weights.shape}\\nWeights: {weights}')\n",
    "                layer.weight = nn.parameter.Parameter(weights)\n",
    "                quantized_lst.append(count)\n",
    "                print(f'1.Layer {count} finished quantization')\n",
    "            else:\n",
    "                # print(\"##1\",layer.weight)\n",
    "                weights = layer.weight.detach()\n",
    "                print(f'2. Layer {count}')# weights shape pre-quantization: {layer.weight.shape}\\nWeights: {weights}')\n",
    "                weights = quantize_dequantize_se(weights,curr_path,count)\n",
    "                # for row in tqdm(range(0,weights.shape[0])):\n",
    "                #     weights[row] = quantize_dequantize_dt(weights[row])\n",
    "                # print(\"##2\",weights)\n",
    "                layer.weight = nn.parameter.Parameter(weights)\n",
    "                # print(f'Layer {count} weights shape post-quantization: {layer.weight.shape}\\nWeights: {weights}')\n",
    "                # print(layer.weight)\n",
    "                # print(\"##3\",layer.weight)\n",
    "                quantized_lst.append(count)\n",
    "                print(f'2.Layer {count} finished quantization')\n",
    "\n",
    "        except ( AttributeError):\n",
    "            pass\n",
    "else:\n",
    "    count = 0\n",
    "    for layer in vgg16.classifier:\n",
    "        count += 1\n",
    "        try:\n",
    "            if len(layer.weight.shape) == 4:\n",
    "                weights = layer.weight.detach()\n",
    "                print(f'Layer {count} weights shape pre-quantization: {weights.shape}')\n",
    "                \n",
    "                for filter in range(0, layer.weight.shape[0]):\n",
    "                    for channel in range(0, layer.weight.shape[1]):\n",
    "                        # print(layer.weight[filter,channel])\n",
    "                        weights[filter,channel] = quantize_dequantize_se(weights[filter,channel])\n",
    "                print(f'Layer {count} weights shape post-quantization: {weights.shape}')\n",
    "                layer.weight = nn.parameter.Parameter(weights)\n",
    "            else:\n",
    "                print(f'In else loop')\n",
    "                print(f'Layer {count} weights shape pre-quantization: {layer.weight.shape}')\n",
    "                intermediate = quantize_dequantize_se(layer.weight.detach())\n",
    "                print(intermediate.shape)\n",
    "                layer.weight = nn.parameter.Parameter(intermediate)\n",
    "                print(f'Layer {count} weights shape post-quantization: {layer.weight.shape}')\n",
    "                # print(layer.weight)\n",
    "        except (TypeError, AttributeError):\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8d5a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'{curr_path}/Stable_Embedding_model'\n",
    "torch.save(vgg16.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf6ff5bc-f132-40a9-922b-79684bba442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <E459C462-F863-3A5A-AC9F-FD77B14BE845> /Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <E459C462-F863-3A5A-AC9F-FD77B14BE845> /Users/yanran/miniconda3/envs/hw2/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "100%|██████████| 2500/2500 [01:20<00:00, 31.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87.13 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DT QUANTIZATION\n",
    "if Test:\n",
    "    #Testing Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = vgg16(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
